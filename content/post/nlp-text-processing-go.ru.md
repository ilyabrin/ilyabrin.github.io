---
title: "Natural Language Processing: –æ—Å–Ω–æ–≤—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ Go"
date: 2025-04-27T18:00:00+01:00

author: "Ilya Brin"
categories: ['golang', 'nlp', 'machine-learning']
tags: ['nlp', 'text-processing', 'golang', 'machine-learning', 'tokenization', 'sentiment-analysis', 'text-mining']
---

–ü—Ä–∏–≤–µ—Ç, –ª–∏–Ω–≥–≤–∏—Å—Ç! üëã

–•–æ—á–µ—à—å –Ω–∞—É—á–∏—Ç—å –∫–æ–º–ø—å—é—Ç–µ—Ä **–ø–æ–Ω–∏–º–∞—Ç—å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π —è–∑—ã–∫**? –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–∑—ã–≤—ã –∫–ª–∏–µ–Ω—Ç–æ–≤, –∏–∑–≤–ª–µ–∫–∞—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–ª–∏ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤?

**Natural Language Processing (NLP)** - —ç—Ç–æ –º–∞–≥–∏—è, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ **–ø–æ–ª–µ–∑–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ**. –ò –¥–∞, —ç—Ç–æ –º–æ–∂–Ω–æ –¥–µ–ª–∞—Ç—å –Ω–∞ Go!

–†–∞–∑–±–∏—Ä–∞–µ–º **–æ—Å–Ω–æ–≤—ã NLP**, **–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã** –∏ **—Ä–µ–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã** –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ Go üöÄ

<!--more-->

## 1. –ß—Ç–æ —Ç–∞–∫–æ–µ NLP –∏ –∑–∞—á–µ–º –æ–Ω–æ –Ω—É–∂–Ω–æ

### –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ NLP

**Natural Language Processing** - —ç—Ç–æ –æ–±–ª–∞—Å—Ç—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è —É—á–∏—Ç –∫–æ–º–ø—å—é—Ç–µ—Ä—ã **–ø–æ–Ω–∏–º–∞—Ç—å**, **–∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å** –∏ **–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å** —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π —è–∑—ã–∫.

**–ü—Ä–∏–º–µ—Ä—ã –∑–∞–¥–∞—á NLP:**

- **–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏** - –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π –∏–ª–∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π –æ—Ç–∑—ã–≤?
- **–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π** - –Ω–∞–π—Ç–∏ –∏–º–µ–Ω–∞, –¥–∞—Ç—ã, –º–µ—Å—Ç–∞ –≤ —Ç–µ–∫—Å—Ç–µ
- **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞** - —Å–ø–∞–º –∏–ª–∏ –Ω–µ —Å–ø–∞–º?
- **–ú–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥** - —Å —Ä—É—Å—Å–∫–æ–≥–æ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π
- **–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞** - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –Ω–∞–ø–∏—Å–∞–Ω–∏–µ —Å—Ç–∞—Ç–µ–π

### –†–µ–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è

```go
// –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è NLP –≤ –±–∏–∑–Ω–µ—Å–µ
type NLPApplication struct {
    Name        string
    Description string
    Value       string
}

var applications = []NLPApplication{
    {"–ê–Ω–∞–ª–∏–∑ –æ—Ç–∑—ã–≤–æ–≤", "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤", "–£–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∞"},
    {"–ß–∞—Ç-–±–æ—Ç—ã", "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã", "–°–Ω–∏–∂–µ–Ω–∏–µ –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ –ø–æ–¥–¥–µ—Ä–∂–∫—É"},
    {"–ü–æ–∏—Å–∫ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º", "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π", "–ë—ã—Å—Ç—Ä—ã–π –¥–æ—Å—Ç—É–ø –∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"},
    {"–ú–æ–¥–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞", "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤—ã—è–≤–ª–µ–Ω–∏–µ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏", "–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è —Å—Ä–µ–¥–∞"},
}
```

## 2. –û—Å–Ω–æ–≤—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞

### –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è - —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞—Å—Ç–∏

```go
package nlp

import (
    "regexp"
    "strings"
    "unicode"
)

type Tokenizer struct {
    wordRegex *regexp.Regexp
}

func NewTokenizer() *Tokenizer {
    return &Tokenizer{
        wordRegex: regexp.MustCompile(`\b\w+\b`),
    }
}

func (t *Tokenizer) TokenizeWords(text string) []string {
    text = strings.ToLower(text)
    return t.wordRegex.FindAllString(text, -1)
}

func (t *Tokenizer) TokenizeSentences(text string) []string {
    sentences := regexp.MustCompile(`[.!?]+`).Split(text, -1)
    result := make([]string, 0, len(sentences))
    
    for _, sentence := range sentences {
        sentence = strings.TrimSpace(sentence)
        if sentence != "" {
            result = append(result, sentence)
        }
    }
    
    return result
}
```

### –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞

```go
func (t *Tokenizer) Normalize(text string) string {
    // –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    text = strings.ToLower(text)
    
    // –£–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
    text = regexp.MustCompile(`[^\p{L}\p{N}\s]+`).ReplaceAllString(text, "")
    
    // –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
    text = regexp.MustCompile(`\s+`).ReplaceAllString(text, " ")
    
    return strings.TrimSpace(text)
}

// –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤
var stopWords = map[string]bool{
    "–∏": true, "–≤": true, "–Ω–∞": true, "—Å": true, "–ø–æ": true,
    "–¥–ª—è": true, "–Ω–µ": true, "–æ—Ç": true, "–¥–æ": true, "–∏–∑": true,
    "the": true, "and": true, "or": true, "but": true, "in": true,
    "on": true, "at": true, "to": true, "for": true, "of": true,
}

func RemoveStopWords(tokens []string) []string {
    result := make([]string, 0, len(tokens))
    for _, token := range tokens {
        if !stopWords[token] && len(token) > 2 {
            result = append(result, token)
        }
    }
    return result
}
```

## 3. –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ (Sentiment Analysis)

### –ü—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Ö–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–æ–≤–∞—Ä—è

```go
type SentimentAnalyzer struct {
    positiveWords map[string]int
    negativeWords map[string]int
}

func NewSentimentAnalyzer() *SentimentAnalyzer {
    return &SentimentAnalyzer{
        positiveWords: map[string]int{
            "—Ö–æ—Ä–æ—à–æ": 2, "–æ—Ç–ª–∏—á–Ω–æ": 3, "–ø—Ä–µ–∫—Ä–∞—Å–Ω–æ": 3, "–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ": 2,
            "good": 2, "great": 3, "excellent": 3, "amazing": 3,
            "love": 2, "perfect": 3, "awesome": 3,
        },
        negativeWords: map[string]int{
            "–ø–ª–æ—Ö–æ": -2, "—É–∂–∞—Å–Ω–æ": -3, "–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω–æ": -3, "–∫–æ—à–º–∞—Ä": -2,
            "bad": -2, "terrible": -3, "awful": -3, "hate": -3,
            "horrible": -3, "disgusting": -3,
        },
    }
}

type SentimentResult struct {
    Score     int
    Sentiment string
    Confidence float64
}

func (sa *SentimentAnalyzer) Analyze(text string) SentimentResult {
    tokenizer := NewTokenizer()
    tokens := tokenizer.TokenizeWords(text)
    tokens = RemoveStopWords(tokens)
    
    score := 0
    wordCount := 0
    
    for _, token := range tokens {
        if value, exists := sa.positiveWords[token]; exists {
            score += value
            wordCount++
        }
        if value, exists := sa.negativeWords[token]; exists {
            score += value
            wordCount++
        }
    }
    
    sentiment := "neutral"
    confidence := 0.0
    
    if score > 0 {
        sentiment = "positive"
        confidence = float64(score) / float64(len(tokens))
    } else if score < 0 {
        sentiment = "negative"
        confidence = float64(-score) / float64(len(tokens))
    }
    
    return SentimentResult{
        Score:      score,
        Sentiment:  sentiment,
        Confidence: confidence,
    }
}
```

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏

```go
func main() {
    analyzer := NewSentimentAnalyzer()
    
    reviews := []string{
        "–≠—Ç–æ—Ç –ø—Ä–æ–¥—É–∫—Ç –ø—Ä–æ—Å—Ç–æ –æ—Ç–ª–∏—á–Ω—ã–π! –û—á–µ–Ω—å –¥–æ–≤–æ–ª–µ–Ω –ø–æ–∫—É–ø–∫–æ–π.",
        "–£–∂–∞—Å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –¥–µ–Ω—å–≥–∏ –Ω–∞ –≤–µ—Ç–µ—Ä. –ù–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é.",
        "–û–±—ã—á–Ω—ã–π —Ç–æ–≤–∞—Ä, –Ω–∏—á–µ–≥–æ –æ—Å–æ–±–µ–Ω–Ω–æ–≥–æ.",
        "Amazing product! Love it so much!",
    }
    
    for _, review := range reviews {
        result := analyzer.Analyze(review)
        fmt.Printf("–û—Ç–∑—ã–≤: %s\n", review)
        fmt.Printf("–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: %s (%.2f)\n\n", result.Sentiment, result.Confidence)
    }
}
```

## 4. TF-IDF –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤

### –†–µ–∞–ª–∏–∑–∞—Ü–∏—è TF-IDF

```go
import "math"

type TFIDFAnalyzer struct {
    documents [][]string
    vocabulary map[string]int
}

func NewTFIDFAnalyzer() *TFIDFAnalyzer {
    return &TFIDFAnalyzer{
        documents: make([][]string, 0),
        vocabulary: make(map[string]int),
    }
}

func (tfidf *TFIDFAnalyzer) AddDocument(tokens []string) {
    tfidf.documents = append(tfidf.documents, tokens)
    
    // –û–±–Ω–æ–≤–ª—è–µ–º —Å–ª–æ–≤–∞—Ä—å
    for _, token := range tokens {
        tfidf.vocabulary[token]++
    }
}

func (tfidf *TFIDFAnalyzer) CalculateTF(tokens []string) map[string]float64 {
    tf := make(map[string]float64)
    totalWords := len(tokens)
    
    for _, token := range tokens {
        tf[token]++
    }
    
    for token := range tf {
        tf[token] = tf[token] / float64(totalWords)
    }
    
    return tf
}

func (tfidf *TFIDFAnalyzer) CalculateIDF(term string) float64 {
    documentsWithTerm := 0
    
    for _, doc := range tfidf.documents {
        for _, token := range doc {
            if token == term {
                documentsWithTerm++
                break
            }
        }
    }
    
    if documentsWithTerm == 0 {
        return 0
    }
    
    return math.Log(float64(len(tfidf.documents)) / float64(documentsWithTerm))
}

func (tfidf *TFIDFAnalyzer) GetTopKeywords(tokens []string, topN int) []KeywordScore {
    tf := tfidf.CalculateTF(tokens)
    scores := make([]KeywordScore, 0)
    
    for term, tfScore := range tf {
        idf := tfidf.CalculateIDF(term)
        tfidfScore := tfScore * idf
        
        scores = append(scores, KeywordScore{
            Word:  term,
            Score: tfidfScore,
        })
    }
    
    // –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é
    sort.Slice(scores, func(i, j int) bool {
        return scores[i].Score > scores[j].Score
    })
    
    if len(scores) > topN {
        scores = scores[:topN]
    }
    
    return scores
}

type KeywordScore struct {
    Word  string
    Score float64
}
```

## 5. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π (NER)

### –ü—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Ö–æ–¥ —Å —Ä–µ–≥—É–ª—è—Ä–Ω—ã–º–∏ –≤—ã—Ä–∞–∂–µ–Ω–∏—è–º–∏

```go
type EntityExtractor struct {
    patterns map[string]*regexp.Regexp
}

func NewEntityExtractor() *EntityExtractor {
    return &EntityExtractor{
        patterns: map[string]*regexp.Regexp{
            "email":  regexp.MustCompile(`\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b`),
            "phone":  regexp.MustCompile(`\b\d{3}-\d{3}-\d{4}\b|\b\+7\d{10}\b`),
            "date":   regexp.MustCompile(`\b\d{1,2}[./]\d{1,2}[./]\d{4}\b`),
            "money":  regexp.MustCompile(`\$\d+(?:,\d{3})*(?:\.\d{2})?|\d+\s*(?:—Ä—É–±–ª–µ–π|—Ä—É–±\.?|–¥–æ–ª–ª–∞—Ä–æ–≤)`),
            "person": regexp.MustCompile(`\b[–ê-–Ø–Å][–∞-—è—ë]+\s+[–ê-–Ø–Å][–∞-—è—ë]+\b`),
        },
    }
}

type Entity struct {
    Type  string
    Value string
    Start int
    End   int
}

func (ee *EntityExtractor) Extract(text string) []Entity {
    entities := make([]Entity, 0)
    
    for entityType, pattern := range ee.patterns {
        matches := pattern.FindAllStringIndex(text, -1)
        for _, match := range matches {
            entities = append(entities, Entity{
                Type:  entityType,
                Value: text[match[0]:match[1]],
                Start: match[0],
                End:   match[1],
            })
        }
    }
    
    return entities
}
```

## 6. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞

### –ù–∞–∏–≤–Ω—ã–π –±–∞–π–µ—Å–æ–≤—Å–∫–∏–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä

```go
type NaiveBayesClassifier struct {
    classes     map[string]int
    wordCounts  map[string]map[string]int
    totalWords  map[string]int
    vocabulary  map[string]bool
}

func NewNaiveBayesClassifier() *NaiveBayesClassifier {
    return &NaiveBayesClassifier{
        classes:    make(map[string]int),
        wordCounts: make(map[string]map[string]int),
        totalWords: make(map[string]int),
        vocabulary: make(map[string]bool),
    }
}

func (nb *NaiveBayesClassifier) Train(text string, class string) {
    tokenizer := NewTokenizer()
    tokens := tokenizer.TokenizeWords(text)
    tokens = RemoveStopWords(tokens)
    
    nb.classes[class]++
    
    if nb.wordCounts[class] == nil {
        nb.wordCounts[class] = make(map[string]int)
    }
    
    for _, token := range tokens {
        nb.wordCounts[class][token]++
        nb.totalWords[class]++
        nb.vocabulary[token] = true
    }
}

func (nb *NaiveBayesClassifier) Predict(text string) string {
    tokenizer := NewTokenizer()
    tokens := tokenizer.TokenizeWords(text)
    tokens = RemoveStopWords(tokens)
    
    bestClass := ""
    bestScore := math.Inf(-1)
    
    totalDocs := 0
    for _, count := range nb.classes {
        totalDocs += count
    }
    
    for class := range nb.classes {
        score := math.Log(float64(nb.classes[class]) / float64(totalDocs))
        
        for _, token := range tokens {
            wordCount := nb.wordCounts[class][token]
            totalWordsInClass := nb.totalWords[class]
            vocabularySize := len(nb.vocabulary)
            
            // –õ–∞–ø–ª–∞—Å–æ–≤—Å–∫–æ–µ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ
            probability := float64(wordCount+1) / float64(totalWordsInClass+vocabularySize)
            score += math.Log(probability)
        }
        
        if score > bestScore {
            bestScore = score
            bestClass = class
        }
    }
    
    return bestClass
}
```

## 7. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–∏–º–µ—Ä: –∞–Ω–∞–ª–∏–∑ –æ—Ç–∑—ã–≤–æ–≤

### –ü–æ–ª–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∞–Ω–∞–ª–∏–∑–∞

```go
type ReviewAnalyzer struct {
    sentiment   *SentimentAnalyzer
    classifier  *NaiveBayesClassifier
    extractor   *EntityExtractor
    tfidf       *TFIDFAnalyzer
}

func NewReviewAnalyzer() *ReviewAnalyzer {
    return &ReviewAnalyzer{
        sentiment:  NewSentimentAnalyzer(),
        classifier: NewNaiveBayesClassifier(),
        extractor:  NewEntityExtractor(),
        tfidf:      NewTFIDFAnalyzer(),
    }
}

type ReviewAnalysis struct {
    Text       string
    Sentiment  SentimentResult
    Category   string
    Keywords   []KeywordScore
    Entities   []Entity
}

func (ra *ReviewAnalyzer) AnalyzeReview(text string) ReviewAnalysis {
    tokenizer := NewTokenizer()
    tokens := tokenizer.TokenizeWords(text)
    cleanTokens := RemoveStopWords(tokens)
    
    return ReviewAnalysis{
        Text:      text,
        Sentiment: ra.sentiment.Analyze(text),
        Category:  ra.classifier.Predict(text),
        Keywords:  ra.tfidf.GetTopKeywords(cleanTokens, 5),
        Entities:  ra.extractor.Extract(text),
    }
}

// –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
func main() {
    analyzer := NewReviewAnalyzer()
    
    // –û–±—É—á–∞–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
    analyzer.classifier.Train("–û—Ç–ª–∏—á–Ω—ã–π —Ç–æ–≤–∞—Ä, –±—ã—Å—Ç—Ä–∞—è –¥–æ—Å—Ç–∞–≤–∫–∞", "positive")
    analyzer.classifier.Train("–ü–ª–æ—Ö–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é", "negative")
    
    // –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è TF-IDF
    tokenizer := NewTokenizer()
    doc1 := RemoveStopWords(tokenizer.TokenizeWords("–û—Ç–ª–∏—á–Ω—ã–π —Ç–æ–≤–∞—Ä"))
    analyzer.tfidf.AddDocument(doc1)
    
    // –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ—Ç–∑—ã–≤
    review := "–ó–∞–∫–∞–∑–∞–ª 15.01.2025, —Ç–æ–≤–∞—Ä –ø—Ä–∏—à–µ–ª –±—ã—Å—Ç—Ä–æ. –ö–∞—á–µ—Å—Ç–≤–æ –æ—Ç–ª–∏—á–Ω–æ–µ! –†–µ–∫–æ–º–µ–Ω–¥—É—é –≤—Å–µ–º. –ú–æ–π email: test@example.com"
    
    result := analyzer.AnalyzeReview(review)
    
    fmt.Printf("–ê–Ω–∞–ª–∏–∑ –æ—Ç–∑—ã–≤–∞:\n")
    fmt.Printf("–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: %s (%.2f)\n", result.Sentiment.Sentiment, result.Sentiment.Confidence)
    fmt.Printf("–ö–∞—Ç–µ–≥–æ—Ä–∏—è: %s\n", result.Category)
    fmt.Printf("–ù–∞–π–¥–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏:\n")
    for _, entity := range result.Entities {
        fmt.Printf("  %s: %s\n", entity.Type, entity.Value)
    }
}
```

## 8. –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

### –ë–µ–Ω—á–º–∞—Ä–∫–∏

```go
func BenchmarkTokenization(b *testing.B) {
    tokenizer := NewTokenizer()
    text := "–≠—Ç–æ –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –≤ Go"
    
    b.ResetTimer()
    for i := 0; i < b.N; i++ {
        tokenizer.TokenizeWords(text)
    }
}

func BenchmarkSentimentAnalysis(b *testing.B) {
    analyzer := NewSentimentAnalyzer()
    text := "–û—Ç–ª–∏—á–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç, –æ—á–µ–Ω—å –¥–æ–≤–æ–ª–µ–Ω –ø–æ–∫—É–ø–∫–æ–π, —Ä–µ–∫–æ–º–µ–Ω–¥—É—é –≤—Å–µ–º"
    
    b.ResetTimer()
    for i := 0; i < b.N; i++ {
        analyzer.Analyze(text)
    }
}

// –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:
// BenchmarkTokenization-8      1000000    1200 ns/op
// BenchmarkSentimentAnalysis-8  500000    2400 ns/op
```

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

```go
// –ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
type CachedAnalyzer struct {
    analyzer *SentimentAnalyzer
    cache    map[string]SentimentResult
    mu       sync.RWMutex
}

func (ca *CachedAnalyzer) Analyze(text string) SentimentResult {
    ca.mu.RLock()
    if result, exists := ca.cache[text]; exists {
        ca.mu.RUnlock()
        return result
    }
    ca.mu.RUnlock()
    
    result := ca.analyzer.Analyze(text)
    
    ca.mu.Lock()
    ca.cache[text] = result
    ca.mu.Unlock()
    
    return result
}
```

## 9. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –≤–Ω–µ—à–Ω–∏–º–∏ API

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Google Translate API

```go
type TranslationService struct {
    apiKey string
    client *http.Client
}

func (ts *TranslationService) Translate(text, targetLang string) (string, error) {
    url := fmt.Sprintf("https://translation.googleapis.com/language/translate/v2?key=%s", ts.apiKey)
    
    payload := map[string]interface{}{
        "q":      text,
        "target": targetLang,
        "format": "text",
    }
    
    jsonData, _ := json.Marshal(payload)
    resp, err := ts.client.Post(url, "application/json", bytes.NewBuffer(jsonData))
    if err != nil {
        return "", err
    }
    defer resp.Body.Close()
    
    var result struct {
        Data struct {
            Translations []struct {
                TranslatedText string `json:"translatedText"`
            } `json:"translations"`
        } `json:"data"`
    }
    
    json.NewDecoder(resp.Body).Decode(&result)
    
    if len(result.Data.Translations) > 0 {
        return result.Data.Translations[0].TranslatedText, nil
    }
    
    return "", fmt.Errorf("no translation found")
}
```

## –í—ã–≤–æ–¥: NLP –Ω–∞ Go - —ç—Ç–æ —Ä–µ–∞–ª—å–Ω–æ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ

**–ß—Ç–æ –º—ã –∏–∑—É—á–∏–ª–∏:**
üî§ **–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è** - —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞—Å—Ç–∏  
üòä **–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏** - –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —ç–º–æ—Ü–∏–π  
üîç **TF-IDF** - –ø–æ–∏—Å–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤  
üè∑Ô∏è **NER** - –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π  
üìä **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è** - –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞  

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ Go –¥–ª—è NLP:**

- **–í—ã—Å–æ–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å** - –±—ã—Å—Ç—Ä–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ —Ç–µ–∫—Å—Ç–∞
- **–ü—Ä–æ—Å—Ç–æ—Ç–∞ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è** - –æ–¥–∏–Ω –±–∏–Ω–∞—Ä–Ω—ã–π —Ñ–∞–π–ª
- **–û—Ç–ª–∏—á–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ concurrency** - –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
- **–ë–æ–≥–∞—Ç–∞—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞** - regexp, strings, unicode

**–°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:**

- –ò–∑—É—á–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏: `prose`, `go-nlp`, `gse`
- –ü–æ–ø—Ä–æ–±—É–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å ML-–º–æ–¥–µ–ª—è–º–∏ —á–µ—Ä–µ–∑ gRPC
- –†–µ–∞–ª–∏–∑—É–π word embeddings —Å Word2Vec

**P.S. –ö–∞–∫–∏–µ NLP-–∑–∞–¥–∞—á–∏ —Ä–µ—à–∞–µ—Ç–µ –≤—ã? –î–µ–ª–∏—Ç–µ—Å—å –æ–ø—ã—Ç–æ–º!** üöÄ

```go
// –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã:
// - "Speech and Language Processing" - Jurafsky & Martin
// - Go NLP libraries: github.com/jdkato/prose
// - Stanford NLP Course: cs224n.stanford.edu
```
